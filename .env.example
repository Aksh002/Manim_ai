# App
LLM_PROVIDER=hf_router
LLM_REQUEST_TIMEOUT_SEC=120
# Hugging Face Router (OpenAI-compatible, recommended)
HF_ROUTER_BASE_URL=https://router.huggingface.co/v1
HF_API_TOKEN=
HF_MODEL_ID=Qwen/Qwen2.5-Coder-7B-Instruct:nscale
HF_MAX_NEW_TOKENS=1400
# Hugging Face dedicated Inference Endpoint (optional)
HF_ENDPOINT_URL=
# Ollama (optional fallback)
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=deepseek-coder:1.3b
# Note: deepseek-coder:6.7b usually needs >5.5 GiB RAM available to Ollama.
# Optional OpenAI fallback settings
OPENAI_API_KEY=
OPENAI_MODEL=gpt-5-mini
API_PORT=8000
WEB_PORT=3000

# Redis / Queue
REDIS_URL=redis://redis:6379/0
USE_QUEUE=true

# Rendering
VIDEO_STORAGE_ROOT=/data/videos
RENDER_TIMEOUT_SEC=120
MAX_RENDER_RETRIES=2
RENDER_MODE=docker
DEFAULT_RENDER_QUALITY=1080p30

# Sandbox limits
RENDERER_IMAGE=manim-ai-renderer:latest
SANDBOX_CPU=1.0
SANDBOX_MEMORY=1g
SANDBOX_PIDS_LIMIT=256
SANDBOX_READ_ONLY=true
SANDBOX_NETWORK_DISABLED=true
SANDBOX_NO_NEW_PRIVILEGES=true
SANDBOX_SECCOMP_PROFILE=/app/app/sandbox/seccomp/renderer-seccomp.json

# API
CORS_ORIGINS=http://localhost:3000
RATE_LIMIT_PER_MIN=60
